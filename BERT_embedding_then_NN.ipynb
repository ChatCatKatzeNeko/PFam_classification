{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another approch to tackle the protein classification problem, this time using deep learning algorithms. The main idea here is to make use of pre-trained protein language model for amino acid embedding. \n",
    "\n",
    "I use the BERT-based protein language model which embeds an amino acid into a float vector of length 768, a model that achieves promising results in several tasks, as described in the article by Rao et al [\\[1\\]](https://arxiv.org/abs/1906.08230). Since it is Transformer-based, this model and similar have an attention mechanism and may contain richer information about a given amino acid and those to which it pays attention (Vig et al [\\[2\\]](https://arxiv.org/abs/2006.15222)). This will be interesting for the current study since such embedding allows to implicitly include some kind of positional information.\n",
    "\n",
    "The embedding is done on amino acid level but not at the sequence level. In this study, I would like to test 3 ways of vectorising a sequence starting from the BERT-based protein language model embedding:\n",
    "1. the sequence is represented by taking average of the amino acid vectors that compose it, hence is of the same length (768);\n",
    "2. by passing the sequence into an LSTM model, the sequence can be represented by the last hidden state. The length of the sequence representation is then the hidden state dimension of the LSTM model, to be defined by the user;\n",
    "3. similar to 2, but instead of LSTM, an 1D-CNN is used to capture information of the whole sequence, in this case the dimension of final representation vector will depend on the kernel size, the padding size and the stride.\n",
    "\n",
    "The vector representation of the sequences are the input of a dense neural network that is trained predict to which family the protein belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./pkg')\n",
    "from Models import *\n",
    "from DataProcessingHelper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read samples of train data\n",
    "PATH_TO_TRAIN = '../data/train/'\n",
    "train = read_datafiles(PATH_TO_TRAIN)\n",
    "\n",
    "# read samples of dev data\n",
    "PATH_TO_DEV = '../data/dev/'\n",
    "dev = read_datafiles(PATH_TO_DEV)\n",
    "\n",
    "# read samples of test data\n",
    "PATH_TO_TEST = '../data/test/'\n",
    "test = read_datafiles(PATH_TO_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again like the \"feature engineering\" part, I only focus on the most populated classes\n",
    "# composing more than a half of the training set and are mutual in the 3 sets of data\n",
    "\n",
    "trainClasses = set(train.family_accession.unique())\n",
    "devClasses = set(dev.family_accession.unique())\n",
    "testClasses = set(test.family_accession.unique())\n",
    "onlyTrain = trainClasses.difference(devClasses)\n",
    "train = train.loc[~train.family_accession.isin(onlyTrain)]\n",
    "# class occurence counts\n",
    "countFreq = train.family_accession.value_counts()\n",
    "cumsum = countFreq.values.cumsum()\n",
    "cumsumN = cumsum / cumsum.max()\n",
    "\n",
    "mostPopular = countFreq.index[:10]\n",
    "#mostPopular = countFreq.index[:np.where(cumsumN>=0.5)[0][0]]\n",
    "#print(\"%d classes represent more than a half of the training data.\" % len(mostPopular))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSub = train.loc[train.family_accession.isin(mostPopular)]\n",
    "testSub = test.loc[test.family_accession.isin(mostPopular)]\n",
    "devSub = dev.loc[dev.family_accession.isin(mostPopular)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save my poor 16G ram\n",
    "del train, test, dev, trainClasses, testClasses, devClasses\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation\n",
    "# - word lvl: OK by pBert\n",
    "# - sequence lvl: \n",
    "#   1. simple mean (bench mark)\n",
    "#   2. final hidden state of an LSTM\n",
    "#   3. 1D Conv (if I have had more resources (time, teammates, machine)...)\n",
    "# classification\n",
    "#   input the vectorised sequence to a dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainSeqLens = trainSub.sequence.apply(len)\n",
    "testSeqLens = testSub.sequence.apply(len)\n",
    "devSeqLens = devSub.sequence.apply(len)\n",
    "\n",
    "fig,axes = plt.subplots(ncols=3,nrows=1,figsize=(15,5))\n",
    "axes[0].hist(trainSeqLens, bins=100, density=True)\n",
    "axes[0].set_title('sequence length distribution (train)')\n",
    "axes[1].hist(testSeqLens, bins=100, density=True)\n",
    "axes[1].set_title('sequence length distribution (test)')\n",
    "axes[2].hist(devSeqLens, bins=100, density=True)\n",
    "axes[2].set_title('sequence length distribution (dev)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the rest of the study, I will constraint the sequence length to be 250\n",
    "MAX_LEN = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars / config\n",
    "BATCH_SIZE = 32\n",
    "NB_LABELS = trainSub.family_accession.nunique()\n",
    "NB_EPOCHS = 3\n",
    "\n",
    "NN_HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparation for training models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerise string labels\n",
    "numLabelDict = {lab:i for i,lab in enumerate(trainSub.family_accession.unique())}\n",
    "\n",
    "trainSub.insert(0,'label',value=trainSub.family_accession.apply(lambda x:numLabelDict[x]))\n",
    "testSub.insert(0,'label',value=testSub.family_accession.apply(lambda x:numLabelDict[x]))\n",
    "devSub.insert(0,'label',value=devSub.family_accession.apply(lambda x:numLabelDict[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert other data types to torch.Tensor\n",
    "trainLabels = torch.tensor(trainSub.label.values)\n",
    "devLabels = torch.tensor(devSub.label.values)\n",
    "testLabels = torch.tensor(testSub.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainTokensIdx = prepare_data(trainSub.sequence.values, maxLen=MAX_LEN)\n",
    "trainData = create_data_loader(trainTokensIdx, trainLabels, batchSize=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "testTokensIdx = prepare_data(testSub.sequence.values, maxLen=MAX_LEN)\n",
    "testData = create_data_loader(testTokensIdx, testLabels, batchSize=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorise the sequence by taking mean of all words from bert embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanClassifier = MeanNNClassifier(nbLabels=NB_LABELS, nnHiddenSize=NN_HIDDEN_SIZE)\n",
    "meanClassifier, optimizer, scheduler = initialize_classifier(meanClassifier,\n",
    "                                                             batchSize=BATCH_SIZE,\n",
    "                                                             epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   10    |   1.806408   |     -      |     -     |  224.59  \n",
      "   1    |   20    |   1.101012   |     -      |     -     |  204.13  \n",
      "   1    |   30    |   0.763870   |     -      |     -     |  204.43  \n",
      "   1    |   40    |   0.628354   |     -      |     -     |  204.66  \n",
      "   1    |   50    |   0.529015   |     -      |     -     |  204.82  \n",
      "   1    |   60    |   0.458696   |     -      |     -     |  203.91  \n",
      "   1    |   70    |   0.367525   |     -      |     -     |  203.21  \n",
      "   1    |   80    |   0.386059   |     -      |     -     |  204.00  \n",
      "   1    |   90    |   0.377988   |     -      |     -     |  203.68  \n",
      "   1    |   100   |   0.356032   |     -      |     -     |  205.35  \n",
      "   1    |   110   |   0.377124   |     -      |     -     |  203.67  \n",
      "   1    |   120   |   0.371601   |     -      |     -     |  204.01  \n",
      "   1    |   130   |   0.314525   |     -      |     -     |  205.15  \n",
      "   1    |   140   |   0.387224   |     -      |     -     |  205.84  \n",
      "   1    |   150   |   0.367991   |     -      |     -     |  204.14  \n",
      "   1    |   160   |   0.380503   |     -      |     -     |  203.97  \n",
      "   1    |   170   |   0.328092   |     -      |     -     |  203.96  \n",
      "   1    |   180   |   0.317965   |     -      |     -     |  204.37  \n",
      "   1    |   190   |   0.314813   |     -      |     -     |  204.16  \n",
      "   1    |   200   |   0.377498   |     -      |     -     |  204.48  \n",
      "   1    |   210   |   0.373370   |     -      |     -     |  205.05  \n",
      "   1    |   220   |   0.386675   |     -      |     -     |  204.60  \n",
      "   1    |   230   |   0.384658   |     -      |     -     |  204.34  \n",
      "   1    |   240   |   0.343531   |     -      |     -     |  204.22  \n",
      "   1    |   250   |   0.317991   |     -      |     -     |  204.33  \n",
      "   1    |   260   |   0.368353   |     -      |     -     |  204.25  \n",
      "   1    |   270   |   0.376806   |     -      |     -     |  204.63  \n",
      "   1    |   280   |   0.360401   |     -      |     -     |  204.46  \n",
      "   1    |   290   |   0.326945   |     -      |     -     |  204.60  \n",
      "   1    |   300   |   0.350972   |     -      |     -     |  204.22  \n",
      "   1    |   310   |   0.385691   |     -      |     -     |  204.03  \n",
      "   1    |   320   |   0.376002   |     -      |     -     |  204.27  \n",
      "   1    |   330   |   0.375421   |     -      |     -     |  204.42  \n",
      "   1    |   340   |   0.340922   |     -      |     -     |  204.08  \n",
      "   1    |   350   |   0.364830   |     -      |     -     |  203.88  \n",
      "   1    |   360   |   0.346912   |     -      |     -     |  204.37  \n",
      "   1    |   370   |   0.369509   |     -      |     -     |  204.41  \n",
      "   1    |   380   |   0.378734   |     -      |     -     |  204.20  \n",
      "   1    |   390   |   0.350485   |     -      |     -     |  204.53  \n",
      "   1    |   400   |   0.325569   |     -      |     -     |  204.45  \n",
      "   1    |   410   |   0.394040   |     -      |     -     |  204.17  \n",
      "   1    |   420   |   0.338457   |     -      |     -     |  204.29  \n",
      "   1    |   430   |   0.356464   |     -      |     -     |  204.14  \n",
      "   1    |   440   |   0.366877   |     -      |     -     |  204.34  \n",
      "   1    |   450   |   0.350610   |     -      |     -     |  205.48  \n",
      "   1    |   460   |   0.350558   |     -      |     -     |  204.90  \n",
      "   1    |   470   |   0.347463   |     -      |     -     |  204.04  \n",
      "   1    |   480   |   0.340844   |     -      |     -     |  204.30  \n",
      "   1    |   490   |   0.355096   |     -      |     -     |  204.29  \n",
      "   1    |   500   |   0.383296   |     -      |     -     |  204.78  \n",
      "   1    |   510   |   0.369210   |     -      |     -     |  205.00  \n",
      "   1    |   520   |   0.388302   |     -      |     -     |  204.53  \n",
      "   1    |   530   |   0.319078   |     -      |     -     |  187.98  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.421036   |  0.297164  |   96.50   | 12080.69 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   10    |   0.363056   |     -      |     -     |  224.29  \n",
      "   2    |   20    |   0.359487   |     -      |     -     |  204.39  \n",
      "   2    |   30    |   0.345322   |     -      |     -     |  203.75  \n",
      "   2    |   40    |   0.344193   |     -      |     -     |  203.94  \n",
      "   2    |   50    |   0.360200   |     -      |     -     |  203.58  \n",
      "   2    |   60    |   0.387657   |     -      |     -     |  203.95  \n",
      "   2    |   70    |   0.362997   |     -      |     -     |  201.13  \n",
      "   2    |   80    |   0.365597   |     -      |     -     |  203.59  \n",
      "   2    |   90    |   0.368355   |     -      |     -     |  203.81  \n",
      "   2    |   100   |   0.367266   |     -      |     -     |  203.92  \n",
      "   2    |   110   |   0.335374   |     -      |     -     |  204.35  \n",
      "   2    |   120   |   0.346822   |     -      |     -     |  204.18  \n",
      "   2    |   130   |   0.353025   |     -      |     -     |  203.95  \n",
      "   2    |   140   |   0.353058   |     -      |     -     |  204.01  \n",
      "   2    |   150   |   0.351485   |     -      |     -     |  203.93  \n",
      "   2    |   160   |   0.323654   |     -      |     -     |  203.93  \n",
      "   2    |   170   |   0.357287   |     -      |     -     |  203.90  \n",
      "   2    |   180   |   0.327057   |     -      |     -     |  203.93  \n",
      "   2    |   190   |   0.381933   |     -      |     -     |  204.00  \n",
      "   2    |   200   |   0.416302   |     -      |     -     |  204.23  \n",
      "   2    |   210   |   0.333058   |     -      |     -     |  204.05  \n",
      "   2    |   220   |   0.344811   |     -      |     -     |  203.58  \n",
      "   2    |   230   |   0.365994   |     -      |     -     |  203.76  \n",
      "   2    |   240   |   0.342137   |     -      |     -     |  204.06  \n",
      "   2    |   250   |   0.389048   |     -      |     -     |  203.96  \n",
      "   2    |   260   |   0.313873   |     -      |     -     |  204.65  \n",
      "   2    |   270   |   0.378788   |     -      |     -     |  203.94  \n",
      "   2    |   280   |   0.379077   |     -      |     -     |  195.89  \n",
      "   2    |   290   |   0.344367   |     -      |     -     |  202.61  \n",
      "   2    |   300   |   0.339118   |     -      |     -     |  204.34  \n",
      "   2    |   310   |   0.340767   |     -      |     -     |  204.10  \n",
      "   2    |   320   |   0.328755   |     -      |     -     |  204.04  \n",
      "   2    |   330   |   0.343652   |     -      |     -     |  203.98  \n",
      "   2    |   340   |   0.362971   |     -      |     -     |  206.56  \n",
      "   2    |   350   |   0.372048   |     -      |     -     |  204.38  \n",
      "   2    |   360   |   0.406913   |     -      |     -     |  203.82  \n",
      "   2    |   370   |   0.342010   |     -      |     -     |  194.34  \n",
      "   2    |   380   |   0.394906   |     -      |     -     |  203.52  \n",
      "   2    |   390   |   0.352307   |     -      |     -     |  204.21  \n",
      "   2    |   400   |   0.335420   |     -      |     -     |  203.96  \n",
      "   2    |   410   |   0.355124   |     -      |     -     |  204.12  \n",
      "   2    |   420   |   0.362502   |     -      |     -     |  204.33  \n",
      "   2    |   430   |   0.372990   |     -      |     -     |  204.16  \n",
      "   2    |   440   |   0.391569   |     -      |     -     |  203.81  \n",
      "   2    |   450   |   0.363689   |     -      |     -     |  204.01  \n",
      "   2    |   460   |   0.332796   |     -      |     -     |  202.13  \n",
      "   2    |   470   |   0.360290   |     -      |     -     |  203.90  \n",
      "   2    |   480   |   0.344992   |     -      |     -     |  204.35  \n",
      "   2    |   490   |   0.366797   |     -      |     -     |  203.85  \n",
      "   2    |   500   |   0.364214   |     -      |     -     |  204.11  \n",
      "   2    |   510   |   0.338412   |     -      |     -     |  204.16  \n",
      "   2    |   520   |   0.344077   |     -      |     -     |  203.86  \n",
      "   2    |   530   |   0.338818   |     -      |     -     |  187.38  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.356925   |  0.292193  |   96.50   | 12041.94 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |   10    |   0.339398   |     -      |     -     |  224.54  \n",
      "   3    |   20    |   0.316229   |     -      |     -     |  203.93  \n",
      "   3    |   30    |   0.328998   |     -      |     -     |  203.93  \n",
      "   3    |   40    |   0.348227   |     -      |     -     |  203.92  \n",
      "   3    |   50    |   0.325633   |     -      |     -     |  203.98  \n",
      "   3    |   60    |   0.300650   |     -      |     -     |  204.25  \n",
      "   3    |   70    |   0.349090   |     -      |     -     |  203.88  \n",
      "   3    |   80    |   0.334077   |     -      |     -     |  204.49  \n",
      "   3    |   90    |   0.352393   |     -      |     -     |  204.13  \n",
      "   3    |   100   |   0.327368   |     -      |     -     |  203.99  \n",
      "   3    |   110   |   0.377704   |     -      |     -     |  205.59  \n",
      "   3    |   120   |   0.364730   |     -      |     -     |  204.11  \n",
      "   3    |   130   |   0.334627   |     -      |     -     |  203.81  \n",
      "   3    |   140   |   0.346548   |     -      |     -     |  203.43  \n",
      "   3    |   150   |   0.352276   |     -      |     -     |  203.90  \n",
      "   3    |   160   |   0.378905   |     -      |     -     |  204.23  \n",
      "   3    |   170   |   0.387546   |     -      |     -     |  204.31  \n",
      "   3    |   180   |   0.399629   |     -      |     -     |  204.26  \n",
      "   3    |   190   |   0.322897   |     -      |     -     |  204.09  \n",
      "   3    |   200   |   0.351154   |     -      |     -     |  204.12  \n",
      "   3    |   210   |   0.403966   |     -      |     -     |  203.99  \n",
      "   3    |   220   |   0.344758   |     -      |     -     |  199.21  \n",
      "   3    |   230   |   0.378867   |     -      |     -     |  195.48  \n",
      "   3    |   240   |   0.351501   |     -      |     -     |  204.21  \n",
      "   3    |   250   |   0.405841   |     -      |     -     |  204.21  \n",
      "   3    |   260   |   0.359265   |     -      |     -     |  204.19  \n",
      "   3    |   270   |   0.321824   |     -      |     -     |  206.66  \n",
      "   3    |   280   |   0.360190   |     -      |     -     |  203.95  \n",
      "   3    |   290   |   0.329307   |     -      |     -     |  204.17  \n",
      "   3    |   300   |   0.379247   |     -      |     -     |  204.20  \n",
      "   3    |   310   |   0.377219   |     -      |     -     |  204.04  \n",
      "   3    |   320   |   0.365003   |     -      |     -     |  204.01  \n",
      "   3    |   330   |   0.373675   |     -      |     -     |  204.50  \n",
      "   3    |   340   |   0.310064   |     -      |     -     |  204.04  \n",
      "   3    |   350   |   0.340979   |     -      |     -     |  204.17  \n",
      "   3    |   360   |   0.316677   |     -      |     -     |  204.09  \n",
      "   3    |   370   |   0.346679   |     -      |     -     |  204.30  \n",
      "   3    |   380   |   0.406623   |     -      |     -     |  204.18  \n",
      "   3    |   390   |   0.397131   |     -      |     -     |  204.08  \n",
      "   3    |   400   |   0.342399   |     -      |     -     |  203.75  \n",
      "   3    |   410   |   0.354212   |     -      |     -     |  204.19  \n",
      "   3    |   420   |   0.340466   |     -      |     -     |  203.88  \n",
      "   3    |   430   |   0.334664   |     -      |     -     |  203.93  \n",
      "   3    |   440   |   0.385963   |     -      |     -     |  203.74  \n",
      "   3    |   450   |   0.348019   |     -      |     -     |  203.94  \n",
      "   3    |   460   |   0.396276   |     -      |     -     |  199.05  \n",
      "   3    |   470   |   0.402619   |     -      |     -     |  202.37  \n",
      "   3    |   480   |   0.350403   |     -      |     -     |  204.08  \n",
      "   3    |   490   |   0.366045   |     -      |     -     |  203.70  \n",
      "   3    |   500   |   0.365317   |     -      |     -     |  204.51  \n",
      "   3    |   510   |   0.337430   |     -      |     -     |  203.79  \n",
      "   3    |   520   |   0.393576   |     -      |     -     |  208.68  \n",
      "   3    |   530   |   0.439548   |     -      |     -     |  187.23  \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.357774   |  0.303053  |   95.80   | 12053.16 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train_model(meanClassifier, optimizer, scheduler, trainData, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model(meanClassifier,testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### represent the sequence by the last hidden state of an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmClassifier = LstmNNClassifier(nbLabels=NB_LABELS, nnHiddenSize=NN_HIDDEN_SIZE,\n",
    "                                  lstmHiddenSize=500)\n",
    "lstmClassifier, optimizer, scheduler = initialize_classifier(lstmClassifier,\n",
    "                                                             batchSize=BATCH_SIZE,\n",
    "                                                             epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(lstmClassifier, optimizer, scheduler, trainData, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### represent the sequence by an 1D-CNN + Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnClassifier = CnnNNClassifier(nbLabels=NB_LABELS, sequenceLen=MAX_LEN, nnHiddenSize=NN_HIDDEN_SIZE,\n",
    "                                nbKernels=30, kernelSize=3)\n",
    "cnnClassifier, optimizer, scheduler = initialize_classifier(cnnClassifier,\n",
    "                                                            batchSize=BATCH_SIZE,\n",
    "                                                            epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(cnnClassifier, optimizer, scheduler, trainData, testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main obstacle for me to complete the study is mainly the number of parametres to be learned compare to the data availability. Let's take a closer look at the first senario, where no parametres to be learned for sequence representation. To train a 1-hidden-layer dense neural network, whose input dimension is 768 and output dimension is about $O(1e5)$, or $O(1e3)$ if we add some constraints on class population. Suppose the hidden layer is of dimension of $O(1e3)$, that gives $768 \\times 1000 + 1000 \\times 1000 = O(1e6)$ trainable model parametres (Note that the training set in its totality contains \"merely\" about $1e6$ records). Not only it demands powerful computational capacity, but also huge amount of labeled training data to have the model be reasonably trained. Let alone the two remaining senarios where even more parametres are needed for the sequence representation. The huge liberty degree of the models can result training failures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
