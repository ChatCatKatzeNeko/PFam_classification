{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another approch to tackle the protein classification problem, this time using deep learning algorithms. The main idea here is to make use of pre-trained protein language model for amino acid embedding. \n",
    "\n",
    "I use the BERT-based protein language model which embeds an amino acid into a float vector of length 768, a model that achieves promising results in several tasks, as described in the article by Rao et al [\\[1\\]](https://arxiv.org/abs/1906.08230). Since it is Transformer-based, this model and similar have an attention mechanism and may contain richer information about a given amino acid and those to which it pays attention (Vig et al [\\[2\\]](https://arxiv.org/abs/2006.15222)). This will be interesting for the current study since such embedding allows to implicitly include some kind of positional information.\n",
    "\n",
    "The embedding is done on amino acid level but not at the sequence level. In this study, I would like to test 3 ways of vectorising a sequence starting from the BERT-based protein language model embedding:\n",
    "1. the sequence is represented by taking average of the amino acid vectors that compose it, hence is of the same length (768);\n",
    "2. by passing the sequence into an LSTM model, the sequence can be represented by the last hidden state. The length of the sequence representation is then the hidden state dimension of the LSTM model, to be defined by the user;\n",
    "3. similar to 2, but instead of LSTM, an 1D-CNN is used to capture information of the whole sequence, in this case the dimension of final representation vector will depend on the kernel size, the padding size and the stride.\n",
    "\n",
    "The vector representation of the sequences are the input of a dense neural network that is trained predict to which family the protein belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./pkg')\n",
    "from Models import *\n",
    "from DataProcessingHelper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read samples of train data\n",
    "PATH_TO_TRAIN = '../data/train/'\n",
    "train = read_datafiles(PATH_TO_TRAIN)\n",
    "\n",
    "# read samples of dev data\n",
    "PATH_TO_DEV = '../data/dev/'\n",
    "dev = read_datafiles(PATH_TO_DEV)\n",
    "\n",
    "# read samples of test data\n",
    "PATH_TO_TEST = '../data/test/'\n",
    "test = read_datafiles(PATH_TO_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again like the \"feature engineering\" part, I only focus on the most populated classes\n",
    "# composing more than a half of the training set and are mutual in the 3 sets of data\n",
    "\n",
    "trainClasses = set(train.family_accession.unique())\n",
    "devClasses = set(dev.family_accession.unique())\n",
    "testClasses = set(test.family_accession.unique())\n",
    "onlyTrain = trainClasses.difference(devClasses)\n",
    "train = train.loc[~train.family_accession.isin(onlyTrain)]\n",
    "# class occurence counts\n",
    "countFreq = train.family_accession.value_counts()\n",
    "cumsum = countFreq.values.cumsum()\n",
    "cumsumN = cumsum / cumsum.max()\n",
    "\n",
    "mostPopular = countFreq.index[:10]\n",
    "#mostPopular = countFreq.index[:np.where(cumsumN>=0.5)[0][0]]\n",
    "#print(\"%d classes represent more than a half of the training data.\" % len(mostPopular))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSub = train.loc[train.family_accession.isin(mostPopular)]\n",
    "testSub = test.loc[test.family_accession.isin(mostPopular)]\n",
    "devSub = dev.loc[dev.family_accession.isin(mostPopular)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save my poor 16G ram\n",
    "del train, test, dev, trainClasses, testClasses, devClasses\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation\n",
    "# - word lvl: OK by pBert\n",
    "# - sequence lvl: \n",
    "#   1. simple mean (bench mark)\n",
    "#   2. final hidden state of an LSTM\n",
    "#   3. 1D Conv (if I have had more resources (time, teammates, machine)...)\n",
    "# classification\n",
    "#   input the vectorised sequence to a dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainSeqLens = trainSub.sequence.apply(len)\n",
    "testSeqLens = testSub.sequence.apply(len)\n",
    "devSeqLens = devSub.sequence.apply(len)\n",
    "\n",
    "fig,axes = plt.subplots(ncols=3,nrows=1,figsize=(15,5))\n",
    "axes[0].hist(trainSeqLens, bins=100, density=True)\n",
    "axes[0].set_title('sequence length distribution (train)')\n",
    "axes[1].hist(testSeqLens, bins=100, density=True)\n",
    "axes[1].set_title('sequence length distribution (test)')\n",
    "axes[2].hist(devSeqLens, bins=100, density=True)\n",
    "axes[2].set_title('sequence length distribution (dev)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the rest of the study, I will constraint the sequence length to be 250\n",
    "MAX_LEN = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars / config\n",
    "BATCH_SIZE = 32\n",
    "NB_LABELS = trainSub.family_accession.nunique()\n",
    "NB_EPOCHS = 3\n",
    "\n",
    "NN_HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparation for training models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerise string labels\n",
    "numLabelDict = {lab:i for i,lab in enumerate(trainSub.family_accession.unique())}\n",
    "\n",
    "trainSub.insert(0,'label',value=trainSub.family_accession.apply(lambda x:numLabelDict[x]))\n",
    "testSub.insert(0,'label',value=testSub.family_accession.apply(lambda x:numLabelDict[x]))\n",
    "devSub.insert(0,'label',value=devSub.family_accession.apply(lambda x:numLabelDict[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert other data types to torch.Tensor\n",
    "trainLabels = torch.tensor(trainSub.label.values)\n",
    "devLabels = torch.tensor(devSub.label.values)\n",
    "testLabels = torch.tensor(testSub.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainTokensIdx = prepare_data(trainSub.sequence.values, maxLen=MAX_LEN)\n",
    "trainData = create_data_loader(trainTokensIdx, trainLabels, batchSize=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "testTokensIdx = prepare_data(testSub.sequence.values, maxLen=MAX_LEN)\n",
    "testData = create_data_loader(testTokensIdx, testLabels, batchSize=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorise the sequence by taking mean of all words from bert embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanClassifier = MeanNNClassifier(nbLabels=NB_LABELS, nnHiddenSize=NN_HIDDEN_SIZE)\n",
    "meanClassifier, optimizer, scheduler = initialize_classifier(meanClassifier,\n",
    "                                                             batchSize=BATCH_SIZE,\n",
    "                                                             epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(meanClassifier, optimizer, scheduler, trainData, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### represent the sequence by the last hidden state of an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 567/567 [00:00<00:00, 118517.41B/s]\n",
      "100%|██████████| 370264230/370264230 [00:47<00:00, 7803747.24B/s] \n"
     ]
    }
   ],
   "source": [
    "lstmClassifier = LstmNNClassifier(nbLabels=NB_LABELS, nnHiddenSize=NN_HIDDEN_SIZE,\n",
    "                                  lstmHiddenSize=500)\n",
    "lstmClassifier, optimizer, scheduler = initialize_classifier(lstmClassifier,\n",
    "                                                             batchSize=BATCH_SIZE,\n",
    "                                                             epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   10    |   2.300053   |     -      |     -     |  260.79  \n",
      "   1    |   20    |   2.291721   |     -      |     -     |  241.51  \n",
      "   1    |   30    |   2.291488   |     -      |     -     |  265.87  \n",
      "   1    |   40    |   2.287117   |     -      |     -     |  596.98  \n",
      "   1    |   50    |   2.288307   |     -      |     -     |  237.62  \n",
      "   1    |   60    |   2.276187   |     -      |     -     |  240.20  \n",
      "   1    |   70    |   2.283416   |     -      |     -     |  240.09  \n",
      "   1    |   80    |   2.263982   |     -      |     -     |  236.07  \n",
      "   1    |   90    |   2.262642   |     -      |     -     |  240.41  \n",
      "   1    |   100   |   2.270820   |     -      |     -     |  239.50  \n",
      "   1    |   110   |   2.280418   |     -      |     -     |  266.07  \n",
      "   1    |   120   |   2.268796   |     -      |     -     |  236.83  \n",
      "   1    |   130   |   2.263921   |     -      |     -     |  236.61  \n",
      "   1    |   140   |   2.278112   |     -      |     -     |  238.77  \n",
      "   1    |   150   |   2.267263   |     -      |     -     |  236.28  \n",
      "   1    |   160   |   2.258800   |     -      |     -     |  234.96  \n",
      "   1    |   170   |   2.280876   |     -      |     -     |  239.43  \n",
      "   1    |   180   |   2.263072   |     -      |     -     |  241.88  \n",
      "   1    |   190   |   2.266160   |     -      |     -     |  238.69  \n",
      "   1    |   200   |   2.281175   |     -      |     -     |  240.19  \n",
      "   1    |   210   |   2.270183   |     -      |     -     |  238.46  \n",
      "   1    |   220   |   2.269183   |     -      |     -     |  235.43  \n",
      "   1    |   230   |   2.278502   |     -      |     -     |  238.13  \n",
      "   1    |   240   |   2.277309   |     -      |     -     |  236.65  \n",
      "   1    |   250   |   2.274965   |     -      |     -     |  238.69  \n",
      "   1    |   260   |   2.277383   |     -      |     -     |  235.19  \n",
      "   1    |   270   |   2.266645   |     -      |     -     |  238.34  \n",
      "   1    |   280   |   2.270652   |     -      |     -     |  234.55  \n",
      "   1    |   290   |   2.277702   |     -      |     -     |  238.05  \n",
      "   1    |   300   |   2.255011   |     -      |     -     |  236.63  \n",
      "   1    |   310   |   2.283061   |     -      |     -     |  235.50  \n",
      "   1    |   320   |   2.277174   |     -      |     -     |  240.87  \n",
      "   1    |   330   |   2.278647   |     -      |     -     |  238.22  \n",
      "   1    |   340   |   2.268929   |     -      |     -     |  244.81  \n",
      "   1    |   350   |   2.274762   |     -      |     -     |  238.74  \n",
      "   1    |   360   |   2.261283   |     -      |     -     |  238.09  \n",
      "   1    |   370   |   2.269980   |     -      |     -     |  236.31  \n",
      "   1    |   380   |   2.279166   |     -      |     -     |  237.79  \n",
      "   1    |   390   |   2.264405   |     -      |     -     |  238.55  \n",
      "   1    |   400   |   2.270823   |     -      |     -     |  235.13  \n",
      "   1    |   410   |   2.276093   |     -      |     -     |  236.14  \n",
      "   1    |   420   |   2.275856   |     -      |     -     |  238.68  \n",
      "   1    |   430   |   2.264508   |     -      |     -     |  239.14  \n",
      "   1    |   440   |   2.269457   |     -      |     -     |  236.82  \n",
      "   1    |   450   |   2.268080   |     -      |     -     |  236.58  \n",
      "   1    |   460   |   2.270907   |     -      |     -     |  243.98  \n",
      "   1    |   470   |   2.278759   |     -      |     -     |  241.88  \n",
      "   1    |   480   |   2.274405   |     -      |     -     |  240.67  \n",
      "   1    |   490   |   2.268371   |     -      |     -     |  245.16  \n",
      "   1    |   500   |   2.276629   |     -      |     -     |  236.49  \n",
      "   1    |   510   |   2.270357   |     -      |     -     |  236.39  \n",
      "   1    |   520   |   2.275425   |     -      |     -     |  237.84  \n",
      "   1    |   530   |   2.276244   |     -      |     -     |  216.67  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   2.273921   |  2.271121  |   20.90   | 14485.53 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   10    |   2.263631   |     -      |     -     |  260.97  \n",
      "   2    |   20    |   2.270753   |     -      |     -     |  239.99  \n",
      "   2    |   30    |   2.270445   |     -      |     -     |  238.88  \n",
      "   2    |   40    |   2.272358   |     -      |     -     |  238.28  \n",
      "   2    |   50    |   2.276415   |     -      |     -     |  244.19  \n",
      "   2    |   60    |   2.262220   |     -      |     -     |  241.87  \n",
      "   2    |   70    |   2.277542   |     -      |     -     |  239.62  \n",
      "   2    |   80    |   2.278034   |     -      |     -     |  234.77  \n",
      "   2    |   90    |   2.263651   |     -      |     -     |  234.59  \n",
      "   2    |   100   |   2.266748   |     -      |     -     |  238.42  \n",
      "   2    |   110   |   2.276720   |     -      |     -     |  240.04  \n",
      "   2    |   120   |   2.272688   |     -      |     -     |  238.76  \n",
      "   2    |   130   |   2.265220   |     -      |     -     |  236.07  \n",
      "   2    |   140   |   2.277440   |     -      |     -     |  241.44  \n",
      "   2    |   150   |   2.260836   |     -      |     -     |  240.77  \n",
      "   2    |   160   |   2.267760   |     -      |     -     |  242.79  \n",
      "   2    |   170   |   2.283793   |     -      |     -     |  237.63  \n",
      "   2    |   180   |   2.282278   |     -      |     -     |  242.15  \n",
      "   2    |   190   |   2.265011   |     -      |     -     |  240.70  \n",
      "   2    |   200   |   2.260428   |     -      |     -     |  242.79  \n",
      "   2    |   210   |   2.260336   |     -      |     -     |  250.10  \n",
      "   2    |   220   |   2.262388   |     -      |     -     |  242.88  \n",
      "   2    |   230   |   2.271024   |     -      |     -     |  239.67  \n",
      "   2    |   240   |   2.275892   |     -      |     -     |  240.05  \n",
      "   2    |   250   |   2.272239   |     -      |     -     |  238.31  \n",
      "   2    |   260   |   2.271644   |     -      |     -     |  239.84  \n",
      "   2    |   270   |   2.280634   |     -      |     -     |  236.58  \n",
      "   2    |   280   |   2.267646   |     -      |     -     |  242.31  \n",
      "   2    |   290   |   2.280603   |     -      |     -     |  237.47  \n",
      "   2    |   300   |   2.264963   |     -      |     -     |  237.16  \n",
      "   2    |   310   |   2.264292   |     -      |     -     |  240.36  \n",
      "   2    |   320   |   2.282234   |     -      |     -     |  237.77  \n",
      "   2    |   330   |   2.262032   |     -      |     -     |  238.57  \n",
      "   2    |   340   |   2.268581   |     -      |     -     |  239.32  \n",
      "   2    |   350   |   2.270669   |     -      |     -     |  245.06  \n",
      "   2    |   360   |   2.270801   |     -      |     -     |  239.63  \n",
      "   2    |   370   |   2.273817   |     -      |     -     |  241.71  \n",
      "   2    |   380   |   2.264854   |     -      |     -     |  237.09  \n",
      "   2    |   390   |   2.278234   |     -      |     -     |  238.91  \n",
      "   2    |   400   |   2.262445   |     -      |     -     |  242.66  \n",
      "   2    |   410   |   2.280854   |     -      |     -     |  244.46  \n",
      "   2    |   420   |   2.269525   |     -      |     -     |  245.57  \n",
      "   2    |   430   |   2.266307   |     -      |     -     |  238.66  \n",
      "   2    |   440   |   2.274258   |     -      |     -     |  239.44  \n",
      "   2    |   450   |   2.256707   |     -      |     -     |  240.57  \n",
      "   2    |   460   |   2.270534   |     -      |     -     |  241.18  \n",
      "   2    |   470   |   2.282477   |     -      |     -     |  239.10  \n",
      "   2    |   480   |   2.275741   |     -      |     -     |  238.96  \n",
      "   2    |   490   |   2.265834   |     -      |     -     |  239.81  \n",
      "   2    |   500   |   2.270738   |     -      |     -     |  236.92  \n",
      "   2    |   510   |   2.276105   |     -      |     -     |  240.63  \n",
      "   2    |   520   |   2.286330   |     -      |     -     |  238.76  \n",
      "   2    |   530   |   2.283183   |     -      |     -     |  218.54  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   2.271267   |  2.267292  |   21.36   | 14116.27 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |   10    |   2.283855   |     -      |     -     |  262.27  \n",
      "   3    |   20    |   2.275361   |     -      |     -     |  239.39  \n",
      "   3    |   30    |   2.273748   |     -      |     -     |  239.89  \n",
      "   3    |   40    |   2.259929   |     -      |     -     |  238.97  \n",
      "   3    |   50    |   2.272264   |     -      |     -     |  235.61  \n",
      "   3    |   60    |   2.263865   |     -      |     -     |  242.23  \n",
      "   3    |   70    |   2.263860   |     -      |     -     |  236.94  \n",
      "   3    |   80    |   2.276695   |     -      |     -     |  238.28  \n",
      "   3    |   90    |   2.279367   |     -      |     -     |  238.78  \n",
      "   3    |   100   |   2.268514   |     -      |     -     |  236.32  \n",
      "   3    |   110   |   2.272352   |     -      |     -     |  237.80  \n",
      "   3    |   120   |   2.276473   |     -      |     -     |  239.19  \n",
      "   3    |   130   |   2.278408   |     -      |     -     |  239.81  \n",
      "   3    |   140   |   2.263778   |     -      |     -     |  239.71  \n",
      "   3    |   150   |   2.278316   |     -      |     -     |  238.53  \n",
      "   3    |   160   |   2.265588   |     -      |     -     |  239.00  \n",
      "   3    |   170   |   2.290338   |     -      |     -     |  241.83  \n",
      "   3    |   180   |   2.278328   |     -      |     -     |  241.03  \n",
      "   3    |   190   |   2.263626   |     -      |     -     |  238.66  \n",
      "   3    |   200   |   2.266935   |     -      |     -     |  237.08  \n",
      "   3    |   210   |   2.256033   |     -      |     -     |  238.98  \n",
      "   3    |   220   |   2.259502   |     -      |     -     |  239.85  \n",
      "   3    |   230   |   2.264095   |     -      |     -     |  239.54  \n",
      "   3    |   240   |   2.269161   |     -      |     -     |  240.14  \n",
      "   3    |   250   |   2.264723   |     -      |     -     |  241.12  \n",
      "   3    |   260   |   2.271304   |     -      |     -     |  241.83  \n",
      "   3    |   270   |   2.260174   |     -      |     -     |  239.37  \n",
      "   3    |   280   |   2.280420   |     -      |     -     |  242.96  \n",
      "   3    |   290   |   2.264800   |     -      |     -     |  238.11  \n",
      "   3    |   300   |   2.262048   |     -      |     -     |  238.56  \n",
      "   3    |   310   |   2.271803   |     -      |     -     |  247.11  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstmClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestData\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workdir/python/code/pkg/Models.py:236\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, trainData, valData, epochs)\u001b[0m\n\u001b[1;32m    233\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Perform a forward pass. This will return res.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatchIdx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Compute loss and accumulate the loss values\u001b[39;00m\n\u001b[1;32m    239\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_cross_entropy(res, batchLabels)\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Workdir/python/code/pkg/Models.py:75\u001b[0m, in \u001b[0;36mNNClassifier.forward\u001b[0;34m(self, tokenIdx)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenIdx):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Feed input to BERT and the classifier to compute logits.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    INPUT\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    res (torch.Tensor): an output tensor with shape (batchSize, nbLabels)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     sequenceRepresentation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenIdx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Feed input to classifier to compute prediction result\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequenceRepresentation)\n",
      "File \u001b[0;32m~/Workdir/python/code/pkg/Models.py:139\u001b[0m, in \u001b[0;36mLstmNNClassifier.represent_sequence\u001b[0;34m(self, tokenIdx)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresent_sequence\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenIdx):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Feed input to BERT => embedded sequence of shape (batch size, sequence length + 2, 768)\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     bertOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenIdx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    140\u001b[0m     representationModel \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;66;03m# 768 = embedding dim of bert\u001b[39;00m\n\u001b[1;32m    142\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;241m768\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstmHiddenSize, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     _,(representation,_) \u001b[38;5;241m=\u001b[39m representationModel(bertOutput) \u001b[38;5;66;03m# shape(representation) = (1, batch size, hidden state size)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/tape/models/modeling_bert.py:447\u001b[0m, in \u001b[0;36mProteinBertModel.forward\u001b[0;34m(self, input_ids, input_mask)\u001b[0m\n\u001b[1;32m    444\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10000.0\u001b[39m\n\u001b[1;32m    446\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids)\n\u001b[0;32m--> 447\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    451\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output)\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/tape/models/modeling_bert.py:346\u001b[0m, in \u001b[0;36mProteinBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, chunks)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_hidden_states:\n\u001b[1;32m    344\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 346\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_attentions:\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/tape/models/modeling_bert.py:287\u001b[0m, in \u001b[0;36mProteinBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    285\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(hidden_states, attention_mask)\n\u001b[1;32m    286\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 287\u001b[0m intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    289\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/tape/models/modeling_bert.py:258\u001b[0m, in \u001b[0;36mProteinBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 258\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/py3/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(lstmClassifier, optimizer, scheduler, trainData, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### represent the sequence by an 1D-CNN + Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnClassifier = CnnNNClassifier(nbLabels=NB_LABELS, sequenceLen=MAX_LEN, nnHiddenSize=NN_HIDDEN_SIZE,\n",
    "                                nbKernels=300, kernelSize=3)\n",
    "cnnClassifier, optimizer, scheduler = initialize_classifier(cnnClassifier,\n",
    "                                                            batchSize=BATCH_SIZE,\n",
    "                                                            epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(cnnClassifier, optimizer, scheduler, trainData, testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main obstacle for me to complete the study is mainly the number of parametres to be learned compare to the data availability. Let's take a closer look at the first senario, where no parametres to be learned for sequence representation. To train a 1-hidden-layer dense neural network, whose input dimension is 768 and output dimension is about $O(1e5)$, or $O(1e3)$ if we add some constraints on class population. Suppose the hidden layer is of dimension of $O(1e3)$, that gives $768 \\times 1000 + 1000 \\times 1000 = O(1e6)$ trainable model parametres (Note that the training set in its totality contains \"merely\" about $1e6$ records). Not only it demands powerful computational capacity, but also huge amount of labeled training data to have the model be reasonably trained. Let alone the two remaining senarios where even more parametres are needed for the sequence representation. The huge liberty degree of the models can result training failures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
